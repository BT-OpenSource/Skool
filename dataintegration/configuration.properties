##### CONFIGURATION DETAILS TO BE FILLED BY USER#####
#set flag 1 for import and 2 for export and 3 for files.
import_export_file_flag=1

#####PROPERTIES COMMON TO ALL TRANSPORT TYPE######

#SOURCE_NAME(or Component name)
source_name=BT

##### ENVIRONMENT DETAILS #####
#1 for cluster (as it will be authenticated for Kerberos) and 2 for VM (no kerberos required) Effected elements will be Hive2 JDBC URL in Oozie + sharelib/main class in Hive oozie action
Environment_Details=2
#Will be used in place of hdfs://nameservice1 for hdfs directory operations while applications runs in gateway node
NameNode=hdfs://quickstart.cloudera:8020
#Part taken from beeline connector while logging into cluster 
#Sample (from 1a) - !connect jdbc:hive2://tplhc01c002:10000/default;principal=hive/tplhc01c002.iuser.iroot.adidom.com@IUSER.IROOT.ADIDOM.COM
hive2_jdbc_url=jdbc:hive2://quickstart.cloudera:10000/
#Can be obtained from hive-site.xml in tag - hive.server2.authentication.kerberos.principal
hive2_server_principal=hive/_HOST@IUSER.IROOT.ADIDOM.COM
jobTracker=quickstart.cloudera:8032
#Instance to which user has access to in cluster
cluster_haas_instance_name=HAASDEMO_01

##### ENVIRONMENT DETAILS #####


##### COORDINATOR PARMETERS #####
# Is coordinator required or you want to run it only once
coordinator_required=false
#Start and End time to be used in Coordinator for scheduling Oozie Jobs
#w3c date-time Format: yyyy-mm-ddThh:MMZ
workflow_start_time=2016-02-22T04:00Z
workflow_end_time=2017-01-01T00:30Z
#Time Zone can be changed as per usage
time_zone=Europe/London
concurrency=1
throttle=1
timeout=0
# frequency to be provided in minutes(1440 mins= 1 day)
frequency=10
##### COORDINATOR PARMETERS #####

#####PROPERTIES COMMON TO ALL TRANSPORT TYPE######


##### Field applicable  to both Import and Export##### 
##### Mandatory Fields #####
##### DATABASE DETAILS #####
database_host=bct06104005-scan-oravip.dci.bt.com
datebase_port=61901
database_sid_or_servicename=NMDT3_ANY
database_schemaname=NM
database_username=nm
database_password=FcYaN8sW
database_tablename=INTERFACES
#Source Database details for sqoop import
#oracle-1 and Mysql-2
RDBMS=1
DBDriver=oracle.jdbc.driver.OracleDriver
##### DATABASE DETAILS #####




##### Mandatory Fields #####

#Email address to which Oozie Success/Failure alerts will go to
success_email_id=someone@bt.com
failure_email_id=someone@bt.com
#No of mappers is needed for sqoop import. Can be modified as per need. By default 1 will be used as only 1 records will be imported as a check
no_of_mappers=1
#avro=1 and sequence=2 and text=3 and parquet=4
sqoopFileFormat=4


### Only for Import #######
#Needed if number of mappers is left blank(not set)
split_by_column=
#Needed by sqoop for incremental import should be a date column which capture the last updated date
#If left blank each and every time the full table will be imported
last_modified_date_column=MODIFIED_DATE
#Specify comma-separated list of columns from table which needs to be imported else
#all the columns will imported by-default
user_selected_columns=

### Only for Import #######


### Only for Export #######
push_to_staging_table=true
update_database=false
fields_terminated_by=
lines_terminated_by=
#HDFS source path for the export.   
export_dir=/user/HAASDEMO_01/EMP/landing/DELTA_DATA/2016/02/22/04/43
#Anchor column to use for updates. Use a comma separated list of columns if there are more than one column.   
update_key_column =
#Specify how updates are performed when new rows are found with non-matching keys in database.Legal values for mode include updateonly (default) and allowinsert.   
update_mode=updateonly
###### NON MANDATORY FIELDS ###########################
# Columns to export to table .It should be comma seperated
columns_name= 
#The table in which data will be staged before being inserted into the destination table.
staging_table =EMPLOYEES
### Only for EXPORT #######

#### HOUSEKEEPING ####
# User should specify the retention period for raw_data in number of days.
retention_period_raw_data=

#### HOUSEKEEPING ####
##### Field applicable  to both Import and Export##### 

##### Field applicable for FileSystem#####
#The directory in cluster where the file resides
file_base_directory=/user/HAASDEMO_01/EMPLOYEES
file_delimiter=[|^]+
file_hive_table_name=EMPLOYEES
control_file_name=cust_success.txt
mapping_sheet_name=cust_mapping_sheet.csv
file_mask=cust*
record_threshold=
file_date_format=
line_number=3
file_trailer_present=true
file_trailer_keyword=FILETRAILER
##### Field applicable for FileSystem##### 
